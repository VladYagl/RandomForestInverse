\chapter{Описание предметной области и анализ существующих решений}\label{chap:first}

\section{Дерево принятия решений}\label{sec:tree}

Дерево принятия решений (\texttt{Decision Tree})\cite{tree} --- модель
применяющаяся в алгоритмах машинного обучения, цель которой состоит в том, чтобы
предсказать значение целевой функции в заданной точке. Структура представляет
собой подвешенное дерево (часто двоичное), где в каждой вершине происходит
разбиение пространства по одному и признаков. Тем самым спускаясь по дереву до
листа, находится в какой области лежит заданная точка (лист дерева
соответствующий данной области), после чего возвращается значение функции
в полученном листе дерева.

Существуют различные алгоритмы для построения дерева, такие как: ID3, C4.5, CART
и другие. Обычно применяется построение сверху вниз, где на каждом шаге
происходит разбиение пространства по некоторому признаку, таким образом чтобы
максимизировать значение выбранной метрики. Часто применяющиеся метрики включают
себя: \emph{критерий Джини}, \emph{информационный выигрыш} и \emph{понижение
дисперсии}. Для избежания переобучения, в конце работы алгоритма используется
\emph{отсечение ветвей} (\texttt{pruning})~\cite{pruning}.

Преимуществами деревьев принятия решения по сравнению с другими моделями
машинного обучения являются:

\begin{enumerate}
    \item Понятная человеку интерпретация процесса принятия решения.
    \item Независимость от характера признаков --- может одинаково работать как
    целочисленными и вещественными признаками, так и с категориальными.
    \item Эффективная скорость обучения, даже при большом наборе данных.
\end{enumerate}

\section{Случайный регрессионный лес}\label{sec:random_forest}

\begin{figure}[ht!]
\caption{Разбиение пространства случайным лесом}\label{random_forest}
\includegraphics[height=0.35\textheight]{random_forest.png}
\end{figure}

Случайный лес (\texttt{Random Forest}\cite{randomforest}) --- модель машинного
обучения, основанная на применении ансамбля деревьев принятия решения, где
каждое дерево обучается независимо от остальных. Итоговый результат получается
как среднее (взвешенное среднее) по все ответам отдельных деревьев. Так как
каждое дерево разбивает пространство на прямоугольные области, где оно
возвращает одинаковые значения, то итоговый лес является пересечением всех таких
разбиений~\cref{random_forest}. Случайный лес называется регрессионным, если он
используется для решения задач предсказания некоторого численного значения.

Для обучения регрессионного леса, исходные входные данные разбиваются на случайные
подвыборки с повторениями. После чего, на каждой выборке обучается отдельное
дерево принятия решений. Так же применяется метод \texttt{feature
bagging}~\cite{bagging}, где деревья обучаются не на полном наборе признаков,
а только на случено выбранном его подмножестве. В реализациях случайного леса
часто обучают каждое дерево до полного истощения данных, то есть пока каждый
лист не соответствует равно одному элементу из набора. Так же обычно не
применяется усечение ветвей, так как в случае ансамбля деревьев, нет
необходимости избегать переобучения конкретного дерева.

\section{Постановка задачи}\label{sec:task}

В данной работе рассматривается задача \emph{оптимизации случайного
регрессионного леса}.

Оптимизация --- задача нахождения экстремума (минимума или максимума) целевой
функци в некоторой области. Так как случайный лес разбивает пространство на
конечное число областей, то имеет место случай комбинаторной
оптимизации~\cite{optimize}.

Постановка Задачи: дан заранее обученный случайный лес. Необходимо найти области
пространства признаков в которых данный случайный лес возвращает свои
максимальное/минимальное значения.

\section{Актуальность поставленной задачи}\label{sec:super_task}

Частое практическое применение случайного регрессионного леса это  предсказание
некоторой неизвестной функции по набору её значений. Это предсказание может
применяться в качестве суррогатной модели в других алгоритмах машинного
обучения. Одним из примеров такого применения является бесовская оптимизация.

Суррогатная модель~\cite{genasurr} применяется когда стоит задача оптимизировать
сложно измеримую величину, вычисление которой требует большого количества
ресурсов. В таком случае вместо оптимизации заданной величины, применяется
оптимизации некоторой модели, которую проще измерить, и оптимизации которой
будет приводить к оптимизации изначальной величины. В качестве такой суррогатной
модели на практике может применяться случайный регрессионной
лес~\cite{surrogate}.

Байесовская оптимизация (Bayesian Optimization),~\cite{bayes1}. Данный
метод строит регрессионную модель на основе результатов запусков настраиваемого
алгоритма. Эта модель позволяет предсказать, где достигается минимум
оптимизируемой функции. Для уточнения предсказаний вблизи некоторой точки,
настраиваемый алгоритм запускается снова, позволяя до-обучить модель.

Для выбора точек, в которых необходимо уточнить модель, можно воспользоваться
понятием \emph{ожидаемого улучшения} (\emph{expected improvement,
EI})~\cite{globalopt}, которое также известно как \emph{верхняя доверительная
граница} Гауссовского процесса (\emph{upper confidence bound,
UCB})~\cite{gaussbandit}.

Частным случаем Байесовской оптимизации является Последовательная Оптимизация,
Основанная на Модели (Sequental Model-Based Optimization, SMBO)~\cite{smac}. Ее
частным случаем является алгоритм Sequential Model-based Algorithm Configuration
(SMAC)~\cite{elf}. Этот алгоритм нам особо интересен, так как в его авторской
реализации применяется случайный лес в качестве регрессионной
модели~\cite{automl_smac_repo}.

Далее а работе произведено исследование эффективности предложенных методов
решения поставленной задачи в приложении на описанный выше метод алгоритм
SMAC\@.

\section{Существующие решения}\label{sec:solutions}

Так как функция задаваемая случайным лесом является трудно обратимой,
в настоящее время методы для её оптимизации не рассматривают внутреннюю
структуру случайного леса, а применяются общие методы для оптимизаций суррогатных
функции.

В данном случае основные подходы это:
\begin{enumerate}
    \item Случайный поиск~\cite{random} --- выдирается случайная точка в пространстве,
    вычисляется значение функции в данной точке. Если полученный ответ лучше
    найденного ранее, то значение ответа обновляется.
    \item Локальный поиск --- поиск начинается из заданной точки, и на каждом шаге
    алгоритм переходит в соседнюю с лучшим значением оптимизируемой функции. Поиск
    останавливается при достижении локального экстремума или по истечении
    установленного времени.
\end{enumerate}

Также применяется комбинация этих двух алгоритмов, где производится локальный
поиск из набора случайных стартовых точек~\cite{local}.

\section{Метод имитации отжига}\label{sec:od}

Популярным методом для решения задач оптимизации является метод имитации
отжига~\cite{od}. Это общий алгоритмический метод решения задачи глобальной
оптимизации, особенно дискретной или комбинаторной. Основной идеей алгоритма,
является имитация физического процесса, происходящего при отжиге металлов,
откуда алгоритм и берет своё название.

Алгоритм производит случайные мутации по переходу из рассматриваемой точки
в соседнюю. При этом, переход в точку с худшим значением целевой функции
осуществляется с вероятностью, постепенно убывающей в соответствии с понижением
температуры:

\[
P(x_i \to x_{i+1}) =
\begin{cases}
    1,                                                  & F(x_i+1) > F(x_i) \\
    \exp \left(-\dfrac{F(x_{i+1})-F(x_i)}{T_i}\right),  & F(x_i+1) \geqslant F(x_i)
\end{cases}
\]

Где $P(x_i \to x_{i+1})$ --- вероятность события перехода в из точки $x_i$
в точку $x_{i+1}$. $F$ --- целевая функция. $T_i > 0$ --- убывающий
температурный параметр.

Основное преимущество данного алгоритма по сравнению с похожими методами
(локальный поиск) --- это случайность при переходе в следующее значение. Что
позволяет избегать остановки алгоритма в локальных экстремумах функции.

\section{Метод ветвей и границ}\label{sec:branch_bound}

Метод ветвей и границ (\texttt{branch and bound}~\cite{branch}) --- ещё один
подход для решения задач комбинаторной оптимизации. Основывается на методе
полного перебора, с отсечением тех вариантов, которые гарантированно не могут
улучшить найденное значение.

Основная идея метода заключается в том, что при некоторых ограничениях на
параметры целевой функции можно оценить её возможные значения. Тем самым можно
не перебирать те границы в которых гарантированно нет максимума функции, а те
границы в которых он потенциально может быть разбиваются на меньшие участки
и процесс повторяется.

Так как метод является вариацией полного перебора, то его эффективность сильно
зависит от качества оценки функции в заданных границах и правильного подбора
этих границ.

В разделе~\ref{sec:heu} описано применение данного метода, для поиска
минимума/максимума в случайном регрессионном дереве. Была изучена литература,
в том числе и современная, но применение метода ветвей и границ для оптимизации
случайного леса нигде не найдено.

\chapterconclusion

В данной главе произведено теоретическое введение в модель машинного обучения
''случайный регрессионный лес'' и входящих в неё компонентов. Поставлена задача,
решаемая в данной работе. 

Также описаны и рассмотрены характеристики алгоритмов оптимизации, которые
используются на практике в данный монумент, и методы использованные в работе.

В данной работе предложен и реализован метод ветвей и границ для оптимизации
функции заданной случайным лесом.
